<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Forrest Glines</title>
    <link>https://forrestglines.github.io/</link>
      <atom:link href="https://forrestglines.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Forrest Glines</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://forrestglines.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Forrest Glines</title>
      <link>https://forrestglines.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://forrestglines.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exascale simulations of magnetized AGN jets on Frontier</title>
      <link>https://forrestglines.github.io/project/incite_2023/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/project/incite_2023/</guid>
      <description>&lt;p&gt;Myself and a group of collaborators  were awarded an &lt;a href=&#34;https://www.anl.gov/article/incite-program-awards-supercomputing-time-to-56-projects-to-accelerate-science-and-engineering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;INCITE 2023
Award&lt;/a&gt;
to perform exascale simulations of galaxy clusters with magnetized jets powered
by a central active galactic nuclei (AGN). Galaxy clusters, as the largest
gravitationally bound structures, provide a unique probe of large scale
structure in the universe. The magnetized AGN jets play a key role in the
dynamics of baryonic matter in galaxy clusters and thus observational
signatures of clusters.&lt;/p&gt;
&lt;p&gt;These simulations are enabled by
&lt;a href=&#34;https://github.com/parthenon-hpc-lab/athenapk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AthenaPK&lt;/a&gt;, our exascale capable
performance portable astrophysics code. I am currently working on optimizing
these AthenaPK simulations on the exascale supercomputer Frontier.&lt;/p&gt;
&lt;p&gt;Read MSU&amp;rsquo;s reporting on the award &lt;a href=&#34;https://www.egr.msu.edu/news/2023/03/22/msu-led-research-team-study-galaxies-never&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AthenaPK and Parthenon</title>
      <link>https://forrestglines.github.io/project/athenapk_and_parthenon/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/project/athenapk_and_parthenon/</guid>
      <description>&lt;p&gt;AthenaPK is an in-development performance-portable conversion of Athena++ build
on the Parthenon adaptive mesh refinement (AMR) framework using the Kokkos
performance portability library. I am one of the main developers for AthenaPK
and a co-developer for Parthenon. The Parthenon framework is designed to be
massively scalable and efficient on both CPUs and GPUs, enabling
next-generation AMR simulations on a variety of hardware architectures. Kernels
and data are managed by Kokkos, which enables high performance on any
architecture supported by Kokkos, including CPUs, NVIDIA and AMD GPUs, and
future Intel GPUs. AthenaPK uses the robust solvers from Athena++ within the
Parthenon framework to enable future exascale astrophysical simulations.&lt;/p&gt;
&lt;p&gt;Parthenon is publicly available on &lt;a href=&#34;https://github.com/lanl/parthenon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;AthenaPK is publicly available on &lt;a href=&#34;https://gitlab.com/theias/hpc/jmstone/athena-parthenon/athenapk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gitlab&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K-Athena</title>
      <link>https://forrestglines.github.io/project/kathena/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/project/kathena/</guid>
      <description>&lt;p&gt;K-Athena is a partial conversion of Athena++, using Kokkos for performance
portability, meaning that it runs efficiently on CPUs and GPUs. The code is a
precursor to the Parthenon and AthenaPK projects, implementing only uniform
grids efficiently when running on GPUs. However, the code was a valuable proof
of concept for a performance-portable magnetohydrodynamics code, allowing
future exascale simulations to be unconstrained by niche architectures.&lt;/p&gt;
&lt;p&gt;K-Athena is publicly available on &lt;a href=&#34;https://gitlab.com/pgrete/kathena&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gitlab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As part of the development effort, we quantified the performance portability of
code using roofline models. We constructed roofline models on each
of the CPU and GPU devices on which we tested K-Athena. Roofline models allow
estimations of the theoretical peak throughput of a code as limited by its
arithmetic intensity (the number of operations execute per byte loaded) and by
the bandwidths and computational throughputs of the hardware. By comparing the
actual efficiency achieved to the theoretical efficiency for each architecture,
we  obtain a performance efficiency for each machine that can be directly
compared, even if the architectures are very different.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/kathena/gpu-roofline_hu92834981f023fb98b90699d135d89f40_39861_97362de59d2e0e0f84704517a9ede06d.png 400w,
               /project/kathena/gpu-roofline_hu92834981f023fb98b90699d135d89f40_39861_13c3c550c23d6f4372a5f9fe3f97a497.png 760w,
               /project/kathena/gpu-roofline_hu92834981f023fb98b90699d135d89f40_39861_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://forrestglines.github.io/project/kathena/gpu-roofline_hu92834981f023fb98b90699d135d89f40_39861_97362de59d2e0e0f84704517a9ede06d.png&#34;
               width=&#34;525&#34;
               height=&#34;390&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Roofline model of an NVIDIA Tesla V100 with the arithmetic intensity of
K-Athena, showing performance in TFLOPS versus arithmetic intensity in floating
point operations execute per byte loaded and written. Throughputs appear as
horizontal ceilings, bandwidths of the different memory spaces of the hardware
appear as diagonal ceilings, and arithmetic intensities of the code appear as
vertical lines. The intersect of an arithmetic intensity with a bandwidth or
throughput ceiling show the theoretical throughput ceiling imposed by that
bandwidth or throughput. We generated rooflines for all architectures on which
we profiled K-Athena.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/kathena/featured_hua9c4d5f5e0b6839025ea1e284575d3fa_28317_9afa34cdd1b87bc61495122d08a7c94d.png 400w,
               /project/kathena/featured_hua9c4d5f5e0b6839025ea1e284575d3fa_28317_83f474163499778ccf150c28cb1a75cb.png 760w,
               /project/kathena/featured_hua9c4d5f5e0b6839025ea1e284575d3fa_28317_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://forrestglines.github.io/project/kathena/featured_hua9c4d5f5e0b6839025ea1e284575d3fa_28317_9afa34cdd1b87bc61495122d08a7c94d.png&#34;
               width=&#34;520&#34;
               height=&#34;409&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Efficiency achieved on each architecture on which we profiled K-Athena, showing
the percentage performance achieved out of the theoretical performance as
limited by the DRAM and L1 memory for each architecture. By taking the harmonic
mean of these efficiencies we arrive at a performance portability measure. The
implementation of K-Athena (and similar MHD codes) is typically limited by the
DRAM bandwidth, leading to a performance portability of 62.8%. Less efficiency
utilization of the L1 cache on almost all architectures leads to a 7.7%
performance portability with respect to the L1 cache.&lt;/p&gt;
&lt;p&gt;Our full method description and performance analysis can be found in
&lt;a href=&#34;https://doi.org/10.1109/TPDS.2020.3010016&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Transactions on Parallel and Distributed Systems&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Magnetized Turbulence in the Taylor-Green Vortex</title>
      <link>https://forrestglines.github.io/project/taylor_green/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/project/taylor_green/</guid>
      <description>&lt;p&gt;To investigate the development of turbulence in intermittently driven plasmas
such as the intracluster medium, we used our newly developed K-Athena code to
run a series of simulations of the magnetized Taylor-Green vortex. The
turbulence that arises from the unsteady flow of the magnetized Taylor-Green
vortex models the turbulence that develops from large scale infrequent events
such as galaxy cluster mergers disturbing the intracluster medium. In our
paper, we examine the magnitude and the spectra of the kinetic, magnetic, and
thermal energy reservoirs within the plasma. Additionally, we apply an energy
transfer analysis to study the movement of energy between different length
scale and between different reservoirs&lt;/p&gt;
&lt;p&gt;Our full setup, explanation, and analysis can be found in
&lt;a href=&#34;https://doi.org/10.1103/PhysRevE.103.043203&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Physical Review E&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Magnetized decaying turbulence in the weakly compressible Taylor-Green vortex </title>
      <link>https://forrestglines.github.io/publication/taylor_green/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/publication/taylor_green/</guid>
      <description>&lt;p&gt;Magnetohydrodynamic (MHD) turbulence affects both terrestrial and astrophysical
plasmas. The properties of magnetized turbulence must be better understood to
more accurately characterize these systems. This work presents ideal MHD
simulations of the compressible Taylor-Green vortex under a range of initial
subsonic Mach numbers and magnetic field strengths. We find that regardless of
the initial field strength, the magnetic energy becomes dominant over the
kinetic energy on all scales after at most several dynamical times. The
spectral indices of the kinetic and magnetic energy spectra become shallower
than $k^{−5/3}$ over time and generally fluctuate. Using a shell-to-shell energy
transfer analysis framework, we find that the magnetic fields facilitate a
significant amount of the energy flux and that the kinetic energy cascade is
suppressed. Moreover, we observe nonlocal energy transfer from the large-scale
kinetic energy to intermediate and small-scale magnetic energy via magnetic
tension. We conclude that even in intermittently or singularly driven weakly
magnetized systems, the dynamical effects of magnetic fields cannot be
neglected.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Environmental Dependence of Self-regulating Black Hole Feedback in Massive Galaxies</title>
      <link>https://forrestglines.github.io/publication/self_regulating_bh_feedback/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/publication/self_regulating_bh_feedback/</guid>
      <description>&lt;p&gt;In the universe&amp;rsquo;s most massive galaxies, active galactic
nucleus (AGN) feedback appears to limit star formation. The
accumulation of cold gas near the central black hole fuels
powerful AGN outbursts, keeping the ambient medium in a
state marginally unstable to condensation and formation of
cold gas clouds. However, the ability of that mechanism to
self-regulate may depend on numerous environmental factors,
including the depth of the potential well and the pressure
of the surrounding circumgalactic medium (CGM). Here we
present a suite of numerical simulations, with halo mass
ranging from $2\times10^{12} M_\odot$ to $8\times10^{14} M_\odot$, exploring the
dependence of AGN feedback on those environmental factors.
We include the spatially extended mass and energy input
from the massive galaxy&amp;rsquo;s old stellar population capable of
sweeping gas out of the galaxy if the confining CGM
pressure is sufficiently low. Our simulations show that
this feedback mechanism is tightly self-regulating in a
massive galaxy with a deep central potential and low CGM
pressure, permitting only small amounts of multiphase gas
to accumulate and allowing no star formation. In a
similar-mass galaxy with shallower central potential and
greater CGM pressure the feedback mechanism is more
episodic, producing extended multiphase gas and allowing
small rates of star formation ($\sim0.1 M_\odot \text{yr}^{−1}$). At the
low-mass end, the mechanism becomes implausibly explosive,
perhaps because the CGM initially has no angular momentum,
which would have reduced the amount of condensed gas
capable of fueling feedback.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tests of AGN Feedback Kernels in Simulated Galaxy Clusters </title>
      <link>https://forrestglines.github.io/publication/agn_thermal/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/publication/agn_thermal/</guid>
      <description>&lt;p&gt;In cool-core galaxy clusters with central cooling times much shorter than a Hubble time, condensation of the ambient central gas is regulated by a heating mechanism, probably an active galactic nucleus. Previous analytical work has suggested that certain radial distributions of heat input may result in convergence to a quasi-steady global state that does not substantively change on the timescale for radiative cooling, even if the heating and cooling are not locally in balance. To test this hypothesis, we simulate idealized galaxy cluster halos using the ENZO code with an idealized, spherically symmetric heat input kernel intended to emulate. Thermal energy is distributed with radius according to a range of kernels, in which total heating is updated to match total cooling every 10 Myr. Some heating kernels can maintain quasi-steady global configurations, but no kernel we tested produces a quasi-steady state with central entropy as low as those observed in cool-core clusters. The general behavior of the simulations depends on the proportion of heating in the inner 10 kpc, with low central heating leading to central cooling catastrophes, high central heating creating a central convective zone with an inverted entropy gradient, and intermediate central heating resulting in a flat central entropy profile that exceeds observations. The timescale on which our simulated halos fall into an unsteady multiphase state is proportional to the square of the cooling time of the lowest-entropy gas, allowing more centrally concentrated heating to maintain a longer-lasting steady state.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> K-Athena: a performance portable structured grid finite volume magnetohydrodynamics code </title>
      <link>https://forrestglines.github.io/publication/kathena/</link>
      <pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/publication/kathena/</guid>
      <description>&lt;p&gt;Large scale simulations are a key pillar of modern research and require
ever-increasing computational resources. Different novel manycore architectures
have emerged in recent years on the way towards the exascale era. Performance
portability is required to prevent repeated non-trivial refactoring of a code
for different architectures. We combine ATHENA++, an existing
magnetohydrodynamics (MHD) CPU code, with KOKKOS, a performance portable
on-node parallel programming paradigm, into K-ATHENA to allow efficient
simulations on multiple architectures using a single codebase. We present
profiling and scaling results for different platforms including Intel Skylake
CPUs, Intel Xeon Phis, and NVIDIA GPUs. K-ATHENA achieves $&amp;gt; 10^8$ cell-updates/s
on a single V100 GPU for second-order double precision MHD calculations, and a
speedup of 30 on up to 24,576 GPUs on Summit (compared to 172,032 CPU cores),
reaching $1:94\times10^12$ total cell-updates/s at 76 percent parallel efficiency.
Using a roofline analysis we demonstrate that the overall performance is
currently limited by DRAM bandwidth and calculate a performance portability
metric of 62.8 percent. Finally, we present the implementation strategies used
and the challenges encountered in maximizing performance. This will provide
other research groups with a straightforward approach to prepare their own
codes for the exascale era. K-ATHENA is available at
&lt;a href=&#34;https://gitlab.com/pgrete/kathena&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gitlab.com/pgrete/kathena&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://forrestglines.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scalable Relativistic High-Resolution Shock-Capturing for Heterogeneous Computing</title>
      <link>https://forrestglines.github.io/publication/scalable_shock_capturing/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/publication/scalable_shock_capturing/</guid>
      <description>&lt;p&gt;A shift is underway in high performance computing (HPC) towards heterogeneous
parallel architectures that emphasize medium and fine grain thread parallelism.
Many scientific computing algorithms, including simple finite-differencing
methods, have already been mapped to heterogeneous architectures with
order-of-magnitude gains in performance as a result. Recent case studies
examining high-resolution shock-capturing (HRSC) algorithms suggest that these
finite-volume methods are good candidates for emerging heterogeneous
architectures. HRSC methods form a key scientific kernel for compressible
inviscid solvers that appear in astrophysics and engineering applications and
tend to require enormous memory and computing resources. This work presents a
case study of an HRSC method executed on a heterogeneous parallel architecture
utilizing hundreds of GPU enabled nodes with remote direct memory access to the
GPUs for a non-trivial shock application using the relativistic
magnetohydrodynamics model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://forrestglines.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://forrestglines.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
